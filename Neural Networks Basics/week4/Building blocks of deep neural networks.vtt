WEBVTT

1
00:00:00.060 --> 00:00:04.830
in the earlier videos from this week as

2
00:00:02.399 --> 00:00:06.810
well as from the videos from the past

3
00:00:04.830 --> 00:00:08.490
several weeks you've already seen the

4
00:00:06.810 --> 00:00:10.710
basic building blocks of board

5
00:00:08.490 --> 00:00:12.929
propagation and back propagation the key

6
00:00:10.710 --> 00:00:15.089
components you need to implement a deep

7
00:00:12.929 --> 00:00:17.130
neural network let's see how you can put

8
00:00:15.089 --> 00:00:19.740
these components together to build a

9
00:00:17.130 --> 00:00:23.369
deep net use the network with a few

10
00:00:19.740 --> 00:00:26.250
layers let's pick one layer and look at

11
00:00:23.369 --> 00:00:29.699
the computations focusing on just that

12
00:00:26.250 --> 00:00:34.079
layer for now so for layer L you have

13
00:00:29.699 --> 00:00:39.230
some parameters WL and Bo and for the

14
00:00:34.079 --> 00:00:42.450
forward prop you will input the

15
00:00:39.230 --> 00:00:49.289
activations a L minus one from the

16
00:00:42.450 --> 00:00:52.710
previous layer and output Al so the way

17
00:00:49.289 --> 00:00:59.600
we did this previously was you compute Z

18
00:00:52.710 --> 00:01:08.280
l equals WL x al minus one plus BL um

19
00:00:59.600 --> 00:01:10.710
and then al equals G of Z L right so

20
00:01:08.280 --> 00:01:13.500
that's how you go from the input al

21
00:01:10.710 --> 00:01:16.320
minus one to the output Al and it turns

22
00:01:13.500 --> 00:01:21.390
out that for later use will be useful to

23
00:01:16.320 --> 00:01:23.909
also cache the value ZL so let me

24
00:01:21.390 --> 00:01:27.119
include this on cache as well because

25
00:01:23.909 --> 00:01:29.820
storing the value Z L will be useful for

26
00:01:27.119 --> 00:01:32.610
backward for the back propagation step

27
00:01:29.820 --> 00:01:34.200
later and then for the backward step or

28
00:01:32.610 --> 00:01:36.329
three for the back propagation step

29
00:01:34.200 --> 00:01:39.509
again focusing on computation for this

30
00:01:36.329 --> 00:01:46.939
layer L you're going to implement a

31
00:01:39.509 --> 00:01:53.130
function that inputs da of L and outputs

32
00:01:46.939 --> 00:01:55.680
da L minus one and just a special the

33
00:01:53.130 --> 00:02:00.450
details the input is actually da FL as

34
00:01:55.680 --> 00:02:03.420
well as the cache so you have available

35
00:02:00.450 --> 00:02:04.400
to you the value of ZL that you compute

36
00:02:03.420 --> 00:02:07.100
it and

37
00:02:04.400 --> 00:02:08.060
in addition to outputting GL minus 1 you

38
00:02:07.100 --> 00:02:10.700
will output

39
00:02:08.060 --> 00:02:12.830
you know the gradients you want in order

40
00:02:10.700 --> 00:02:15.730
to implement gradient descent for

41
00:02:12.830 --> 00:02:18.830
learning ok so this is the basic

42
00:02:15.730 --> 00:02:20.690
structure of how you implement this

43
00:02:18.830 --> 00:02:22.280
forward step I'm going to call it a

44
00:02:20.690 --> 00:02:24.160
forward function as well as backward

45
00:02:22.280 --> 00:02:28.160
step we shall call it back wave function

46
00:02:24.160 --> 00:02:30.530
so just to summarize in layer L you're

47
00:02:28.160 --> 00:02:31.730
going to have you know the forward step

48
00:02:30.530 --> 00:02:38.630
or the forward property' forward

49
00:02:31.730 --> 00:02:42.020
function input a L minus 1 and output al

50
00:02:38.630 --> 00:02:45.410
and in order to make this computation

51
00:02:42.020 --> 00:02:51.770
you need to use WL n PL um and also

52
00:02:45.410 --> 00:02:54.740
output a cache which contains ZL and

53
00:02:51.770 --> 00:02:59.470
then on the backward function using the

54
00:02:54.740 --> 00:03:06.770
back prop step will be another function

55
00:02:59.470 --> 00:03:11.269
then now inputs the AFL and outputs da

56
00:03:06.770 --> 00:03:13.400
ll minus 1 so it tells you given the

57
00:03:11.269 --> 00:03:16.970
derivatives respect to these activations

58
00:03:13.400 --> 00:03:19.760
that's da FL what are the derivatives or

59
00:03:16.970 --> 00:03:22.220
how much do I wish you know a L minus 1

60
00:03:19.760 --> 00:03:24.640
changes computed derivatives respect to

61
00:03:22.220 --> 00:03:27.860
D activations from the previous layer

62
00:03:24.640 --> 00:03:31.070
within this box right you need to use WL

63
00:03:27.860 --> 00:03:35.239
and BL and it turns out along the way

64
00:03:31.070 --> 00:03:38.209
you end up computing DZ l um and then

65
00:03:35.239 --> 00:03:44.660
this box this backward function can also

66
00:03:38.209 --> 00:03:46.790
output DW l and DB l well now sometimes

67
00:03:44.660 --> 00:03:48.610
using red arrows to denote the backward

68
00:03:46.790 --> 00:03:52.459
generations so if you prefer we could

69
00:03:48.610 --> 00:03:55.640
draw these arrows in red so if you can

70
00:03:52.459 --> 00:03:57.799
implement these two functions then the

71
00:03:55.640 --> 00:04:00.440
basic computation of the mirror network

72
00:03:57.799 --> 00:04:04.250
will be as follows you're going to take

73
00:04:00.440 --> 00:04:06.769
the input features a 0 see that in and

74
00:04:04.250 --> 00:04:10.340
that will compute the activations of the

75
00:04:06.769 --> 00:04:15.440
first layer let's call that a 1 and to

76
00:04:10.340 --> 00:04:17.840
do that you needed W 1 and B 1 and then

77
00:04:15.440 --> 00:04:22.790
we'll also you know cache

78
00:04:17.840 --> 00:04:25.639
the way z1 now having done that you feed

79
00:04:22.790 --> 00:04:28.820
that this is the second layer and then

80
00:04:25.639 --> 00:04:31.310
using W 2 and B 2 you're going to

81
00:04:28.820 --> 00:04:37.280
compute the activations our next layer a

82
00:04:31.310 --> 00:04:41.060
2 and so on until eventually you end up

83
00:04:37.280 --> 00:04:47.090
outputting a capital L which is equal to

84
00:04:41.060 --> 00:04:53.479
Y hat and along the way we cashed all of

85
00:04:47.090 --> 00:04:56.210
these on values Z so that's the forward

86
00:04:53.479 --> 00:04:58.790
propagation step now for the back

87
00:04:56.210 --> 00:05:03.169
propagation step what we're going to do

88
00:04:58.790 --> 00:05:06.620
will be a backward sequence of

89
00:05:03.169 --> 00:05:09.080
iterations in which you're going

90
00:05:06.620 --> 00:05:15.550
backwards and computing gradients like

91
00:05:09.080 --> 00:05:21.350
so so as you're going to feed in here da

92
00:05:15.550 --> 00:05:31.240
L and then this box will give us da of L

93
00:05:21.350 --> 00:05:33.770
minus 1 and so on until we get da - da 1

94
00:05:31.240 --> 00:05:37.550
you could actually get one more output

95
00:05:33.770 --> 00:05:39.650
to compute da 0 but this is derivative

96
00:05:37.550 --> 00:05:42.320
respect your input features which is not

97
00:05:39.650 --> 00:05:45.460
useful at least for training the weights

98
00:05:42.320 --> 00:05:48.680
of these are supervised neural networks

99
00:05:45.460 --> 00:05:50.479
so you could just stop it there belong

100
00:05:48.680 --> 00:05:56.180
the way back prop also ends up

101
00:05:50.479 --> 00:06:01.610
outputting DW l DB l right this used

102
00:05:56.180 --> 00:06:11.520
upon so wo and BL um this would output d

103
00:06:01.610 --> 00:06:15.440
w3 g p3 and so on so you enter

104
00:06:11.520 --> 00:06:15.440
computing all the derivatives you need

105
00:06:16.160 --> 00:06:22.259
until just a maybe so in the structure

106
00:06:19.770 --> 00:06:26.300
of this a little bit more right these

107
00:06:22.259 --> 00:06:26.300
boxes will use those parameters of slow

108
00:06:27.139 --> 00:06:34.740
WL PL and it turns out that we'll see

109
00:06:32.340 --> 00:06:37.949
later that inside these boxes we'll end

110
00:06:34.740 --> 00:06:39.720
up computing disease as well so one

111
00:06:37.949 --> 00:06:43.020
innovation of training for a new network

112
00:06:39.720 --> 00:06:46.729
involves starting with a zero which is X

113
00:06:43.020 --> 00:06:49.380
and going through for profit as follows

114
00:06:46.729 --> 00:06:53.580
computing Y hat and then using that to

115
00:06:49.380 --> 00:06:57.539
compute this and then back prop right

116
00:06:53.580 --> 00:07:01.800
doing that and now you have all these

117
00:06:57.539 --> 00:07:04.380
derivative terms and so you know W will

118
00:07:01.800 --> 00:07:07.889
get updated as some W minus the learning

119
00:07:04.380 --> 00:07:14.009
rate times DW right for each of the

120
00:07:07.889 --> 00:07:16.380
layers and similarly for B right now

121
00:07:14.009 --> 00:07:18.389
we've compute the back prop and have all

122
00:07:16.380 --> 00:07:20.550
these derivatives so that's one

123
00:07:18.389 --> 00:07:23.159
iteration of gradient descent for your

124
00:07:20.550 --> 00:07:24.949
neural network now before moving on just

125
00:07:23.159 --> 00:07:27.330
one more implementational detail

126
00:07:24.949 --> 00:07:32.090
conceptually will be useful to think of

127
00:07:27.330 --> 00:07:34.740
the cashier as storing the value of Z

128
00:07:32.090 --> 00:07:36.479
for the backward functions but when you

129
00:07:34.740 --> 00:07:37.740
implement this you see this in the

130
00:07:36.479 --> 00:07:38.190
programming exercise when you implement

131
00:07:37.740 --> 00:07:39.840
it

132
00:07:38.190 --> 00:07:42.029
you find that the cash may be a

133
00:07:39.840 --> 00:07:45.419
convenient way to get the value of the

134
00:07:42.029 --> 00:07:47.069
parameters at W 1 V 1 into the backward

135
00:07:45.419 --> 00:07:50.159
function as well so the program exercise

136
00:07:47.069 --> 00:07:55.669
you actually spawn the cash is Z as well

137
00:07:50.159 --> 00:07:58.949
as W and B all right so to store z2w to

138
00:07:55.669 --> 00:08:00.900
be to go from an implementational

139
00:07:58.949 --> 00:08:03.060
standpoint I just find this a convenient

140
00:08:00.900 --> 00:08:06.300
way to just you know get the parameters

141
00:08:03.060 --> 00:08:07.919
copied to where you need to need to use

142
00:08:06.300 --> 00:08:09.569
them later when you're computing back

143
00:08:07.919 --> 00:08:11.430
propagation so that's just an

144
00:08:09.569 --> 00:08:15.719
implementational detail that you see

145
00:08:11.430 --> 00:08:17.190
when you do the programming exercise so

146
00:08:15.719 --> 00:08:19.169
you've now seen one of the basic

147
00:08:17.190 --> 00:08:20.909
building blocks for implementing a deep

148
00:08:19.169 --> 00:08:22.349
neural network image layer there's a for

149
00:08:20.909 --> 00:08:24.000
propagation step and there's a

150
00:08:22.349 --> 00:08:24.780
corresponding backward propagation step

151
00:08:24.000 --> 00:08:26.970
and as

152
00:08:24.780 --> 00:08:29.400
cash deposit information from one to the

153
00:08:26.970 --> 00:08:31.110
other in the next video we'll talk about

154
00:08:29.400 --> 00:08:33.000
how you can actually implement these

155
00:08:31.110 --> 00:08:35.180
building blocks let's go into the next

156
00:08:33.000 --> 00:08:35.180
video