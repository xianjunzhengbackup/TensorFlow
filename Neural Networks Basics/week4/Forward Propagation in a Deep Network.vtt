WEBVTT

1
00:00:00.060 --> 00:00:04.380
in the last video we distract what is

2
00:00:02.250 --> 00:00:06.150
the deep l-larry neural network and also

3
00:00:04.380 --> 00:00:08.550
talked about the notation we use to

4
00:00:06.150 --> 00:00:10.650
describe such networks in this video you

5
00:00:08.550 --> 00:00:13.769
see how you can perform for propagation

6
00:00:10.650 --> 00:00:16.440
in a deep network as usual let's first

7
00:00:13.769 --> 00:00:18.660
go over what forward propagation will

8
00:00:16.440 --> 00:00:21.330
look like for a single training example

9
00:00:18.660 --> 00:00:22.920
X and then later on we'll talk about the

10
00:00:21.330 --> 00:00:24.810
vectorized version where you want to

11
00:00:22.920 --> 00:00:26.849
carry out forward propagation on the

12
00:00:24.810 --> 00:00:29.660
entire training set at the same time but

13
00:00:26.849 --> 00:00:32.579
some given a single training example X

14
00:00:29.660 --> 00:00:34.800
here's how you compute the activations

15
00:00:32.579 --> 00:00:42.329
of the first layer so for this first

16
00:00:34.800 --> 00:00:48.239
layer you compute z1 equals W 1 times X

17
00:00:42.329 --> 00:00:51.120
plus b1 so W 1 and B 1 of parameters

18
00:00:48.239 --> 00:00:53.879
that affect the activations in layer 1

19
00:00:51.120 --> 00:00:56.899
right where this is there one of the

20
00:00:53.879 --> 00:00:59.280
neural network and then you compute the

21
00:00:56.899 --> 00:01:04.979
activations for that layer to be equal

22
00:00:59.280 --> 00:01:06.810
to G of Z 1 and the destination function

23
00:01:04.979 --> 00:01:09.090
G depends on what layer you're at and

24
00:01:06.810 --> 00:01:11.010
maybe index AB has the activation

25
00:01:09.090 --> 00:01:12.689
function from there 1 so if you do that

26
00:01:11.010 --> 00:01:13.320
you've now computed the activations from

27
00:01:12.689 --> 00:01:18.360
layer 1

28
00:01:13.320 --> 00:01:24.470
how about layer to say that layer well

29
00:01:18.360 --> 00:01:32.189
you would then compute Z 2 equals W 2 A

30
00:01:24.470 --> 00:01:34.950
1 plus B 2 and then so the activation of

31
00:01:32.189 --> 00:01:39.180
layer 2 is the way matrix times the

32
00:01:34.950 --> 00:01:44.270
output of layer 1 so is that value plus

33
00:01:39.180 --> 00:01:49.579
the bias vector for layer 2 and then a2

34
00:01:44.270 --> 00:01:55.770
equals the activation function apply to

35
00:01:49.579 --> 00:01:57.990
z2 ok so that's it for layer 2 and so on

36
00:01:55.770 --> 00:02:00.299
and so forth until you get to the output

37
00:01:57.990 --> 00:02:06.240
layer that's layer 4 where you would

38
00:02:00.299 --> 00:02:09.959
have that Z 4 is equal to the parameters

39
00:02:06.240 --> 00:02:11.780
for that layer times the activations

40
00:02:09.959 --> 00:02:14.569
from the previous layer

41
00:02:11.780 --> 00:02:23.930
Plus that by this vector and then

42
00:02:14.569 --> 00:02:26.720
similarly a four equals G of v4 and so

43
00:02:23.930 --> 00:02:29.900
that's how you compute your estimated

44
00:02:26.720 --> 00:02:35.390
output Y hat so just one thing to notice

45
00:02:29.900 --> 00:02:38.270
X here is also equal to a zero because

46
00:02:35.390 --> 00:02:41.209
the input feature vector X is also the

47
00:02:38.270 --> 00:02:44.000
activations of layer 0 so we scratch out

48
00:02:41.209 --> 00:02:47.000
X and with a cross for X and put a 0

49
00:02:44.000 --> 00:02:48.709
here then you know all of these

50
00:02:47.000 --> 00:02:53.980
equations they see look the same right

51
00:02:48.709 --> 00:03:02.750
the general rule is that VL is equal to

52
00:02:53.980 --> 00:03:05.750
WL times a of L minus 1 plus B L 1 there

53
00:03:02.750 --> 00:03:10.630
and then the activations so that layer

54
00:03:05.750 --> 00:03:16.850
is the activation function applied to

55
00:03:10.630 --> 00:03:20.120
the values Z so that's the general for

56
00:03:16.850 --> 00:03:23.540
propagation equation so we've done all

57
00:03:20.120 --> 00:03:26.299
this for a single training example how

58
00:03:23.540 --> 00:03:29.660
about for doing it in a vectorized way

59
00:03:26.299 --> 00:03:32.720
for the whole training set at the same

60
00:03:29.660 --> 00:03:35.030
time the equations look quite similar as

61
00:03:32.720 --> 00:03:40.060
before for the first layer you would

62
00:03:35.030 --> 00:03:48.410
have Capital Z 1 equals W 1 times

63
00:03:40.060 --> 00:03:54.650
capital X plus B 1 and then a 1 equals G

64
00:03:48.410 --> 00:03:57.920
of Z 1 right and bearing in mind that X

65
00:03:54.650 --> 00:03:59.959
is equal to a 0 these are just you know

66
00:03:57.920 --> 00:04:01.850
the training examples stacked in

67
00:03:59.959 --> 00:04:05.450
different columns you could take this

68
00:04:01.850 --> 00:04:08.269
let me scratch out X so you can put a 0

69
00:04:05.450 --> 00:04:08.720
there and then so the next layer looks

70
00:04:08.269 --> 00:04:16.720
similar

71
00:04:08.720 --> 00:04:21.980
Z 2 equals W 2 A 1 plus B 2 and a 2

72
00:04:16.720 --> 00:04:24.530
equals G of Z 2

73
00:04:21.980 --> 00:04:28.370
we're just taking these vectors e or a

74
00:04:24.530 --> 00:04:29.810
and so on and stacking them up so this

75
00:04:28.370 --> 00:04:34.310
is V vector for the first training

76
00:04:29.810 --> 00:04:37.310
example V vector for the second training

77
00:04:34.310 --> 00:04:39.830
example and so on down to the M train

78
00:04:37.310 --> 00:04:43.700
example and stacking these in columns

79
00:04:39.830 --> 00:04:47.390
and calling this Capital Z all right and

80
00:04:43.700 --> 00:04:50.000
similarly for capital A just as capital

81
00:04:47.390 --> 00:04:52.040
X all the training examples are column

82
00:04:50.000 --> 00:04:53.720
vectors smacks left to right and then

83
00:04:52.040 --> 00:04:59.450
again end of this process you end up

84
00:04:53.720 --> 00:05:03.200
with Y hat which is equal to G of v4 so

85
00:04:59.450 --> 00:05:04.670
this is also equal to a 4 and that's the

86
00:05:03.200 --> 00:05:08.000
predictions on all the view training

87
00:05:04.670 --> 00:05:09.980
examples is stacked horizontally so just

88
00:05:08.000 --> 00:05:12.590
to summarize our notation I'm going to

89
00:05:09.980 --> 00:05:17.720
modify this up here our notation allows

90
00:05:12.590 --> 00:05:19.820
us to replace lowercase Z and a with the

91
00:05:17.720 --> 00:05:22.070
uppercase counterparts is that already

92
00:05:19.820 --> 00:05:23.810
looks like a capital D and that gives

93
00:05:22.070 --> 00:05:25.790
you the vectorized version the fourth

94
00:05:23.810 --> 00:05:29.060
obligation that you carry out on the

95
00:05:25.790 --> 00:05:32.990
entire training set at a time where a 0

96
00:05:29.060 --> 00:05:35.240
is X now if you look at this

97
00:05:32.990 --> 00:05:37.670
implementation of vectorization it looks

98
00:05:35.240 --> 00:05:40.370
like that there is going to be a for

99
00:05:37.670 --> 00:05:44.360
loop here right so it's left for l

100
00:05:40.370 --> 00:05:47.000
equals 1 to 4 for l equals 1 through

101
00:05:44.360 --> 00:05:48.950
capital L then you have to compute the

102
00:05:47.000 --> 00:05:51.860
activations for layer 1 then for layer 2

103
00:05:48.950 --> 00:05:54.370
then to layer 3 and then 4 therefore so

104
00:05:51.860 --> 00:05:56.660
seems that there is a for loop here and

105
00:05:54.370 --> 00:05:58.550
I know that when implementing your

106
00:05:56.660 --> 00:06:00.770
networks we usually want to get rid of

107
00:05:58.550 --> 00:06:03.290
explicit for loops but this is one place

108
00:06:00.770 --> 00:06:05.060
where I don't think there's any way to

109
00:06:03.290 --> 00:06:06.590
implement this over other than explicit

110
00:06:05.060 --> 00:06:09.080
for loop so we're in implementing for

111
00:06:06.590 --> 00:06:10.700
propagation it is perfectly OK to have a

112
00:06:09.080 --> 00:06:12.740
for loop they compute the activations

113
00:06:10.700 --> 00:06:15.050
for layer 1 then there are 2 then they

114
00:06:12.740 --> 00:06:17.210
are threes and therefore no one knows

115
00:06:15.050 --> 00:06:19.970
and I don't think there is this any way

116
00:06:17.210 --> 00:06:23.060
to do this without a for loops that goes

117
00:06:19.970 --> 00:06:24.620
from 1 to capital L from 1 through the

118
00:06:23.060 --> 00:06:27.830
total number of layers and in your

119
00:06:24.620 --> 00:06:30.980
network so this place is perfectly okay

120
00:06:27.830 --> 00:06:32.690
to have an explicit form so

121
00:06:30.980 --> 00:06:35.300
that's it for the notation for deep

122
00:06:32.690 --> 00:06:37.760
neural networks as well as how to do for

123
00:06:35.300 --> 00:06:39.680
propagation in these networks if the

124
00:06:37.760 --> 00:06:41.900
pieces we've seen so far looks a little

125
00:06:39.680 --> 00:06:44.000
bit familiar to you that's because what

126
00:06:41.900 --> 00:06:45.830
we've seen is taking a piece very

127
00:06:44.000 --> 00:06:47.750
similar to what you've seen in the

128
00:06:45.830 --> 00:06:50.750
neural network with a single hidden

129
00:06:47.750 --> 00:06:53.420
layer and just repeating that more times

130
00:06:50.750 --> 00:06:55.420
now turns out that we implemented deep

131
00:06:53.420 --> 00:06:57.860
neural network one of the ways to

132
00:06:55.420 --> 00:06:59.450
increase your odds of having above free

133
00:06:57.860 --> 00:07:01.580
implementation is to think very

134
00:06:59.450 --> 00:07:03.500
systematic and carefully about the

135
00:07:01.580 --> 00:07:05.300
matrix dimensions you're working with so

136
00:07:03.500 --> 00:07:07.280
when I'm trying to develop my own code I

137
00:07:05.300 --> 00:07:08.960
often pull a piece of paper and just

138
00:07:07.280 --> 00:07:11.480
think carefully through so the

139
00:07:08.960 --> 00:07:13.940
dimensions of the matrix I'm working

140
00:07:11.480 --> 00:07:16.570
with let's see how you could do that in

141
00:07:13.940 --> 00:07:16.570
the next video