WEBVTT

1
00:00:00.060 --> 00:00:04.200
in a previous video you saw the basic

2
00:00:02.340 --> 00:00:07.140
blocks of implementing a deep neural

3
00:00:04.200 --> 00:00:08.849
network a for propagation step for each

4
00:00:07.140 --> 00:00:10.860
layer and a corresponding backward

5
00:00:08.849 --> 00:00:12.540
propagation step let's see how you can

6
00:00:10.860 --> 00:00:15.269
actually implement these steps will

7
00:00:12.540 --> 00:00:18.960
start - for propagation recall that what

8
00:00:15.269 --> 00:00:22.350
this will do is input al - 1 and output

9
00:00:18.960 --> 00:00:24.269
al and the cache ZL and we just said

10
00:00:22.350 --> 00:00:28.320
that from implementational point of view

11
00:00:24.269 --> 00:00:30.150
maybe we'll cache WL + BL as well just

12
00:00:28.320 --> 00:00:32.309
to make the assumptions call the easier

13
00:00:30.150 --> 00:00:34.230
in the preliminaries eyes and so the

14
00:00:32.309 --> 00:00:36.660
equations for this should already look

15
00:00:34.230 --> 00:00:42.290
familiar the way to improve the forward

16
00:00:36.660 --> 00:00:49.829
function is just this equals WL x a l

17
00:00:42.290 --> 00:00:54.090
minus 1 plus b l and then al equals the

18
00:00:49.829 --> 00:00:56.390
activation function applied to z and if

19
00:00:54.090 --> 00:01:05.210
you want a vector rise implementation

20
00:00:56.390 --> 00:01:08.850
then it's just that x a l minus 1 plus b

21
00:01:05.210 --> 00:01:13.020
would be adding beeping pipes and

22
00:01:08.850 --> 00:01:17.189
broadcasting and al equals G applied

23
00:01:13.020 --> 00:01:20.460
element wise to Z and remember on the

24
00:01:17.189 --> 00:01:22.920
diagram for the forth step where we have

25
00:01:20.460 --> 00:01:27.450
this chain of boxes going forward so you

26
00:01:22.920 --> 00:01:30.540
initialize that with feeding and a 0

27
00:01:27.450 --> 00:01:32.579
which is equal to X so you initialize

28
00:01:30.540 --> 00:01:36.329
this really what is the input to the

29
00:01:32.579 --> 00:01:39.570
first one right is really on a 0 which

30
00:01:36.329 --> 00:01:40.950
is the input features to either for one

31
00:01:39.570 --> 00:01:44.909
training example if you're doing one

32
00:01:40.950 --> 00:01:46.380
example at a time or um a practical 0

33
00:01:44.909 --> 00:01:48.390
the entire training so if you are

34
00:01:46.380 --> 00:01:50.939
processing the entire training site so

35
00:01:48.390 --> 00:01:52.619
that's the input to the first forward

36
00:01:50.939 --> 00:01:54.270
function in the chain and then just

37
00:01:52.619 --> 00:01:56.399
repeating this allows you to compute

38
00:01:54.270 --> 00:01:58.890
forward propagation from left to right

39
00:01:56.399 --> 00:02:01.380
next let's talk about the backward

40
00:01:58.890 --> 00:02:06.270
propagation step here you go is the

41
00:02:01.380 --> 00:02:09.780
input D al and output D al minus 1 and d

42
00:02:06.270 --> 00:02:12.430
WL & DB let me just write out the steps

43
00:02:09.780 --> 00:02:17.720
you need to compute these things

44
00:02:12.430 --> 00:02:24.860
pz l is equal to da l alamin Weis

45
00:02:17.720 --> 00:02:28.780
product with G of L prime Z of L and

46
00:02:24.860 --> 00:02:34.130
then compute the derivatives DW l equals

47
00:02:28.780 --> 00:02:36.410
d ZL times a of L minus 1

48
00:02:34.130 --> 00:02:37.970
I didn't explicitly put that in the

49
00:02:36.410 --> 00:02:44.690
cache where it turns out you need this

50
00:02:37.970 --> 00:02:51.530
as well and then DB l is equal to DZ l

51
00:02:44.690 --> 00:02:58.910
and finally da of L minus 1 there's

52
00:02:51.530 --> 00:03:01.340
equal to WL transpose times d ZL ok and

53
00:02:58.910 --> 00:03:02.930
I don't want to go through the detailed

54
00:03:01.340 --> 00:03:04.610
derivation for this but it turns out

55
00:03:02.930 --> 00:03:07.580
that if you take this definition to DA

56
00:03:04.610 --> 00:03:09.410
and plug it in here then you get the

57
00:03:07.580 --> 00:03:13.730
same formula as we had in there

58
00:03:09.410 --> 00:03:15.800
previously for how you compute d ZL as a

59
00:03:13.730 --> 00:03:18.650
function of the previous easy L in fact

60
00:03:15.800 --> 00:03:24.410
well if I just plug that in here you end

61
00:03:18.650 --> 00:03:32.959
up that d ZL is equal to WL plus 1

62
00:03:24.410 --> 00:03:35.630
transpose DZ l plus 1 times G L prime Z

63
00:03:32.959 --> 00:03:37.400
FL I know this is a looks like a lot of

64
00:03:35.630 --> 00:03:38.780
algebra and you can actually double

65
00:03:37.400 --> 00:03:40.430
check for yourself that this is the

66
00:03:38.780 --> 00:03:43.160
equation where I've written down for

67
00:03:40.430 --> 00:03:44.720
back propagation last week when we were

68
00:03:43.160 --> 00:03:47.030
doing in your network with just a single

69
00:03:44.720 --> 00:03:49.400
hidden layer and as you reminder this

70
00:03:47.030 --> 00:03:51.950
times this element-wise product but so

71
00:03:49.400 --> 00:03:54.500
all you need is those four equations to

72
00:03:51.950 --> 00:03:57.140
implement your backward function and

73
00:03:54.500 --> 00:03:59.600
then finally I'll just write out the

74
00:03:57.140 --> 00:04:05.269
vectorized version so the first line

75
00:03:59.600 --> 00:04:11.600
becomes DZ l equals e a l element-wise

76
00:04:05.269 --> 00:04:17.720
product with GL prime of z l may be no

77
00:04:11.600 --> 00:04:23.320
surprise there DW l becomes 1 over m DZ

78
00:04:17.720 --> 00:04:29.790
l times a L minus 1 transpose and

79
00:04:23.320 --> 00:04:34.680
in dbl becomes one over m MP dot some

80
00:04:29.790 --> 00:04:38.440
easy L then accrues equals one

81
00:04:34.680 --> 00:04:42.040
keep dims equals true we talked about

82
00:04:38.440 --> 00:04:46.600
the use of MP dot some in the previous

83
00:04:42.040 --> 00:04:56.620
week to compute DB and then finally da L

84
00:04:46.600 --> 00:04:59.290
minus one is WL transpose times T Z L so

85
00:04:56.620 --> 00:05:08.290
this allows you to input this quantity

86
00:04:59.290 --> 00:05:13.450
da over here and output on DW l DP l the

87
00:05:08.290 --> 00:05:16.540
derivatives you need as well as da L

88
00:05:13.450 --> 00:05:19.060
minus 1 right as follows so that's how

89
00:05:16.540 --> 00:05:22.590
you implement the backward function so

90
00:05:19.060 --> 00:05:25.630
just to summarize um take the input X

91
00:05:22.590 --> 00:05:29.530
you might have the first layer maybe has

92
00:05:25.630 --> 00:05:31.540
a regular activation function then go to

93
00:05:29.530 --> 00:05:33.820
the second layer maybe uses another

94
00:05:31.540 --> 00:05:36.550
value activation function goes to the

95
00:05:33.820 --> 00:05:38.080
third layer maybe has a sigmoid

96
00:05:36.550 --> 00:05:40.510
activation function if you're doing

97
00:05:38.080 --> 00:05:44.200
binary classification and this outputs

98
00:05:40.510 --> 00:05:47.530
one hat and then using Y hat you can

99
00:05:44.200 --> 00:05:50.890
compute the loss and this allows you to

100
00:05:47.530 --> 00:05:52.660
start your backward integration I draw

101
00:05:50.890 --> 00:05:57.180
the arrows first I guess I don't have to

102
00:05:52.660 --> 00:05:57.180
change pens too much where you were then

103
00:05:57.360 --> 00:06:15.280
have back prop compute the derivatives

104
00:06:02.830 --> 00:06:17.920
compute your d w3 t p3 d w2 t p2 d w1 t

105
00:06:15.280 --> 00:06:20.500
b1 and along the way you would be

106
00:06:17.920 --> 00:06:26.650
computing I guess the cash will transfer

107
00:06:20.500 --> 00:06:32.360
z1 z2 z3 and here are you pass backward

108
00:06:26.650 --> 00:06:35.400
da 2 and da

109
00:06:32.360 --> 00:06:36.979
this could compute da zero but we won't

110
00:06:35.400 --> 00:06:39.600
use that so you can just discard that

111
00:06:36.979 --> 00:06:42.240
and so this is how you implement forward

112
00:06:39.600 --> 00:06:44.820
prop and back prop for a three-layer

113
00:06:42.240 --> 00:06:46.800
your network now there's just one last

114
00:06:44.820 --> 00:06:49.050
detail delight didn't talk about which

115
00:06:46.800 --> 00:06:52.560
is for the forward recursion we would

116
00:06:49.050 --> 00:06:54.539
initialize it with the input data X how

117
00:06:52.560 --> 00:06:59.850
about the backward recursion well it

118
00:06:54.539 --> 00:07:01.320
turns out that um D a of L when you're

119
00:06:59.850 --> 00:07:04.380
using logistic regression when you're

120
00:07:01.320 --> 00:07:10.169
doing binary qualification is equal to Y

121
00:07:04.380 --> 00:07:11.820
over a plus 1 minus y over 1 minus a so

122
00:07:10.169 --> 00:07:13.710
it turns out that the derivative of the

123
00:07:11.820 --> 00:07:16.410
loss function respect to the output

124
00:07:13.710 --> 00:07:18.510
we're expected Y hat can be shown to be

125
00:07:16.410 --> 00:07:20.490
equal to this so if you're familiar with

126
00:07:18.510 --> 00:07:22.710
calculus if you look up the loss

127
00:07:20.490 --> 00:07:24.780
function L and take derivatives respect

128
00:07:22.710 --> 00:07:27.150
to Y I have a respect to ay you can show

129
00:07:24.780 --> 00:07:29.580
that you get that formula so this is the

130
00:07:27.150 --> 00:07:32.849
formula you should use for da for the

131
00:07:29.580 --> 00:07:34.590
final layer capital L and of course if

132
00:07:32.849 --> 00:07:36.960
you were to have a vectorized

133
00:07:34.590 --> 00:07:39.240
implementation then you initialize the

134
00:07:36.960 --> 00:07:43.080
backward recursion not with this there

135
00:07:39.240 --> 00:07:45.510
will be a capital A for the layer L

136
00:07:43.080 --> 00:07:48.960
which is going to be you know the same

137
00:07:45.510 --> 00:07:51.930
thing for the different examples right

138
00:07:48.960 --> 00:07:54.780
over a for the first training example

139
00:07:51.930 --> 00:07:57.570
plus 1 minus y for the first training

140
00:07:54.780 --> 00:08:01.199
example over 1 minus 8 for the first

141
00:07:57.570 --> 00:08:05.610
training example not down to the M

142
00:08:01.199 --> 00:08:08.370
training example 1 minus a of M so

143
00:08:05.610 --> 00:08:09.810
that's how you taught implement the

144
00:08:08.370 --> 00:08:12.000
vectorized version that's how you

145
00:08:09.810 --> 00:08:14.039
initialize the vectorized version of

146
00:08:12.000 --> 00:08:16.380
background brocation so you've now seen

147
00:08:14.039 --> 00:08:19.130
the basic building blocks of both for

148
00:08:16.380 --> 00:08:22.320
propagation as well as back propagation

149
00:08:19.130 --> 00:08:24.300
now if you implement these equations you

150
00:08:22.320 --> 00:08:26.280
will get a correct implementation of

151
00:08:24.300 --> 00:08:28.380
board prop and back prop to get to the

152
00:08:26.280 --> 00:08:29.729
derivatives unique you might be thinking

153
00:08:28.380 --> 00:08:31.050
well there's a lot equations I'm

154
00:08:29.729 --> 00:08:32.729
slightly confused I'm not quite sure I

155
00:08:31.050 --> 00:08:35.310
see how this works and if you're feeling

156
00:08:32.729 --> 00:08:37.500
that way my advice is when you get to

157
00:08:35.310 --> 00:08:39.630
this week's programming assignment you

158
00:08:37.500 --> 00:08:41.190
will be able to implement these for

159
00:08:39.630 --> 00:08:43.020
yourself and there'll be much more

160
00:08:41.190 --> 00:08:45.400
concrete and I know there's a lot of

161
00:08:43.020 --> 00:08:48.220
equations and maybe some equations in me

162
00:08:45.400 --> 00:08:49.630
complete sense but if you work through

163
00:08:48.220 --> 00:08:51.820
the calculus and the linear algebra

164
00:08:49.630 --> 00:08:53.380
which is not easy so you know feel free

165
00:08:51.820 --> 00:08:55.600
to try but that's actually have one is

166
00:08:53.380 --> 00:08:57.400
more difficult derivations in machine

167
00:08:55.600 --> 00:08:59.110
learning it turns out the equations

168
00:08:57.400 --> 00:09:01.060
wrote down that just the calculus

169
00:08:59.110 --> 00:09:03.550
equations for computing the derivatives

170
00:09:01.060 --> 00:09:04.990
especially in backdrop but once again if

171
00:09:03.550 --> 00:09:07.570
this C is well bit ass check will be

172
00:09:04.990 --> 00:09:09.190
mysterious to you my advice is when

173
00:09:07.570 --> 00:09:11.160
you've done there provide exercise it

174
00:09:09.190 --> 00:09:13.660
will feel a bit more concrete to you

175
00:09:11.160 --> 00:09:15.190
although I have to say you know even

176
00:09:13.660 --> 00:09:17.680
today when I implement a learning

177
00:09:15.190 --> 00:09:18.760
algorithm sometimes even I'm surprised

178
00:09:17.680 --> 00:09:20.290
when my learning algorithm

179
00:09:18.760 --> 00:09:22.540
implementation works and it's because

180
00:09:20.290 --> 00:09:24.790
longer complexity of machine learning

181
00:09:22.540 --> 00:09:26.650
comes from the data rather than from the

182
00:09:24.790 --> 00:09:28.840
lines of code so sometimes you feel like

183
00:09:26.650 --> 00:09:30.490
you implement a few lines of code not

184
00:09:28.840 --> 00:09:32.140
question what it did but there's almost

185
00:09:30.490 --> 00:09:34.450
magically work and it's because of all

186
00:09:32.140 --> 00:09:36.010
the magic is actually not in the piece

187
00:09:34.450 --> 00:09:37.900
of code you write which is often you

188
00:09:36.010 --> 00:09:39.790
know not too long it's not it's not

189
00:09:37.900 --> 00:09:42.400
exactly simple but there's not you know

190
00:09:39.790 --> 00:09:45.160
10,000 100,000 lines of code but you

191
00:09:42.400 --> 00:09:46.270
feed it so much data that sometimes even

192
00:09:45.160 --> 00:09:47.980
though I work the machine only for a

193
00:09:46.270 --> 00:09:49.840
long time sometimes it's so you know

194
00:09:47.980 --> 00:09:51.460
surprises me a bit when my learning

195
00:09:49.840 --> 00:09:53.230
algorithm works because lots of

196
00:09:51.460 --> 00:09:55.710
complexity of your learning algorithm

197
00:09:53.230 --> 00:09:58.330
comes from the data rather than

198
00:09:55.710 --> 00:10:00.300
necessarily from your writing you know

199
00:09:58.330 --> 00:10:04.060
thousands and thousands of lines of code

200
00:10:00.300 --> 00:10:06.250
all right so that's um how do you

201
00:10:04.060 --> 00:10:07.840
implement deep neural networks and again

202
00:10:06.250 --> 00:10:10.570
this will become more concrete when

203
00:10:07.840 --> 00:10:13.720
you've done the priming exercise before

204
00:10:10.570 --> 00:10:16.030
moving on I want to discuss in the next

205
00:10:13.720 --> 00:10:18.490
video want to discuss hyper parameters

206
00:10:16.030 --> 00:10:20.230
and parameters it turns out that when

207
00:10:18.490 --> 00:10:22.510
you're training deep nets being able to

208
00:10:20.230 --> 00:10:24.490
organize your hyper params as well will

209
00:10:22.510 --> 00:10:26.530
help you be more efficient in developing

210
00:10:24.490 --> 00:10:29.940
your networks in the next video let's

211
00:10:26.530 --> 00:10:29.940
talk about exactly what that means