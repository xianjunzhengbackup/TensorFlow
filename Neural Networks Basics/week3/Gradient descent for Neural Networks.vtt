WEBVTT

1
00:00:00.000 --> 00:00:03.840
all right I think that's be an exciting

2
00:00:01.800 --> 00:00:06.240
video in this video you see how to

3
00:00:03.840 --> 00:00:08.730
implement gradient descent for your

4
00:00:06.240 --> 00:00:10.530
neural network with one hidden layer in

5
00:00:08.730 --> 00:00:12.809
this video I'm going to just give you

6
00:00:10.530 --> 00:00:14.639
the equations you need to implement in

7
00:00:12.809 --> 00:00:17.039
order to get back propagation of the

8
00:00:14.639 --> 00:00:19.410
gradient descent working and then in the

9
00:00:17.039 --> 00:00:21.510
video after this one I'll give some more

10
00:00:19.410 --> 00:00:24.150
intuition about why these particular

11
00:00:21.510 --> 00:00:26.070
equations are the accurate equations or

12
00:00:24.150 --> 00:00:27.630
the correct equations for computing the

13
00:00:26.070 --> 00:00:28.289
gradients you need for your neural

14
00:00:27.630 --> 00:00:30.090
network

15
00:00:28.289 --> 00:00:32.520
so your neural network with a single

16
00:00:30.090 --> 00:00:39.930
peasant layer for now will have

17
00:00:32.520 --> 00:00:44.760
parameters W 1 V 1 W 2 and B 2 and so as

18
00:00:39.930 --> 00:00:50.399
a reminder if you have NX alternative

19
00:00:44.760 --> 00:00:56.640
the UM n 0 input features and N 1 hidden

20
00:00:50.399 --> 00:00:59.149
units and n 2 output units in our

21
00:00:56.640 --> 00:01:05.670
example so far don't yet n 2 equals 1

22
00:00:59.149 --> 00:01:08.880
then the matrix W 1 will be N 1 by n 0 V

23
00:01:05.670 --> 00:01:11.250
1 will be an N 1 dimensional vector so

24
00:01:08.880 --> 00:01:13.350
you can write down as a 10-1 by 1

25
00:01:11.250 --> 00:01:16.500
dimensional matrix really a column

26
00:01:13.350 --> 00:01:20.750
vector the dimensions of W 2 will be n 2

27
00:01:16.500 --> 00:01:26.759
by N 1 and the dimension of B 2 will be

28
00:01:20.750 --> 00:01:28.590
n 2 by 1 right where again so far we've

29
00:01:26.759 --> 00:01:30.930
only seen examples where n 2 is equal to

30
00:01:28.590 --> 00:01:36.930
1 where you have just one a single

31
00:01:30.930 --> 00:01:39.570
hidden unit so you also have a cost

32
00:01:36.930 --> 00:01:41.340
function for a neural network and for

33
00:01:39.570 --> 00:01:44.220
now I'm just going to assume that you're

34
00:01:41.340 --> 00:01:48.659
doing binary classification so in that

35
00:01:44.220 --> 00:01:51.740
case the cost of your parameters as

36
00:01:48.659 --> 00:01:57.090
follows is going to be 1 over m of the

37
00:01:51.740 --> 00:01:59.969
average of that loss function and so L

38
00:01:57.090 --> 00:02:03.420
here is the loss when your new network

39
00:01:59.969 --> 00:02:06.240
predicts y hat right this is really a a

40
00:02:03.420 --> 00:02:07.649
2 when the ground should 0 is equal to Y

41
00:02:06.240 --> 00:02:09.629
and if you're doing binary

42
00:02:07.649 --> 00:02:12.510
classification the loss function can be

43
00:02:09.629 --> 00:02:15.030
exactly what you use for logistic

44
00:02:12.510 --> 00:02:18.420
earlier so to train the parameters your

45
00:02:15.030 --> 00:02:21.450
algorithms you need to perform gradient

46
00:02:18.420 --> 00:02:23.189
descent when training a neural network

47
00:02:21.450 --> 00:02:25.379
is important to initialize the

48
00:02:23.189 --> 00:02:26.129
parameters randomly rounded into all

49
00:02:25.379 --> 00:02:28.140
zeros

50
00:02:26.129 --> 00:02:30.030
we'll see later why that's the case but

51
00:02:28.140 --> 00:02:32.069
after initializing the parameter to

52
00:02:30.030 --> 00:02:34.140
something each loop of gradient descent

53
00:02:32.069 --> 00:02:36.780
would compute the predictions

54
00:02:34.140 --> 00:02:42.359
so you basically compute you know y hat

55
00:02:36.780 --> 00:02:44.519
I for I equals 1 through m say and then

56
00:02:42.359 --> 00:02:49.440
you need to compute the derivative so

57
00:02:44.519 --> 00:02:51.420
you need to compute DW 1 and that's we

58
00:02:49.440 --> 00:02:54.359
see the derivative of the cost function

59
00:02:51.420 --> 00:02:56.489
with respect to the parameter W 1 you

60
00:02:54.359 --> 00:02:59.220
need to compute another variable which

61
00:02:56.489 --> 00:03:00.870
is going to call DP 1 which is the

62
00:02:59.220 --> 00:03:04.109
derivative or the slope of your cost

63
00:03:00.870 --> 00:03:07.349
function with respect to the variable B

64
00:03:04.109 --> 00:03:10.170
1 and so on similarly for the other

65
00:03:07.349 --> 00:03:12.629
parameters W 2 and B 2 and then finally

66
00:03:10.170 --> 00:03:17.879
the gradient descent update would be to

67
00:03:12.629 --> 00:03:22.709
update W 1 as W 1 minus alpha the

68
00:03:17.879 --> 00:03:26.129
learning rate times d w1 v 1 gets

69
00:03:22.709 --> 00:03:31.620
updated as B 1 minus the learning rate

70
00:03:26.129 --> 00:03:34.739
times D B 1 as similarly for W 2 and B 2

71
00:03:31.620 --> 00:03:36.299
and sometimes I use colon equals and

72
00:03:34.739 --> 00:03:38.489
sometimes equals as either either the

73
00:03:36.299 --> 00:03:40.829
notation works fine and so this would be

74
00:03:38.489 --> 00:03:42.510
one iteration of gradient descent and

75
00:03:40.829 --> 00:03:44.280
then your repeat this some number of

76
00:03:42.510 --> 00:03:46.079
times until your parameters look like

77
00:03:44.280 --> 00:03:48.150
they're converging so in previous videos

78
00:03:46.079 --> 00:03:50.099
we talked about how to compute the

79
00:03:48.150 --> 00:03:51.629
predictions how to compute the outputs

80
00:03:50.099 --> 00:03:54.060
and we saw how to do that in a

81
00:03:51.629 --> 00:03:56.269
vectorized way as well so the key is to

82
00:03:54.060 --> 00:04:00.180
know how to compute these partial

83
00:03:56.269 --> 00:04:04.079
derivative terms the DW 1 DB 1 as well

84
00:04:00.180 --> 00:04:06.780
as the derivatives BW 2 and DP 2 so what

85
00:04:04.079 --> 00:04:09.419
I'd like to do is just give you the

86
00:04:06.780 --> 00:04:12.150
equations you need in order to compute

87
00:04:09.419 --> 00:04:14.699
these derivatives and I'll defer to the

88
00:04:12.150 --> 00:04:17.430
next video which is an optional video to

89
00:04:14.699 --> 00:04:19.090
go great turn to Jeff about how we came

90
00:04:17.430 --> 00:04:21.400
up with those formulas

91
00:04:19.090 --> 00:04:26.169
so then just summarize again the

92
00:04:21.400 --> 00:04:33.250
equations for for propagation so you

93
00:04:26.169 --> 00:04:37.900
have Z 1 equals W 1 X plus B 1 and then

94
00:04:33.250 --> 00:04:41.680
a 1 equals the activation function in

95
00:04:37.900 --> 00:04:49.690
that layer applied other than Y since V

96
00:04:41.680 --> 00:04:53.530
1 and then Z 2 equals W 2 A 1 plus B 2

97
00:04:49.690 --> 00:04:55.180
and then finally difference all

98
00:04:53.530 --> 00:05:01.210
vectorize across your training set right

99
00:04:55.180 --> 00:05:02.740
a 2 is equal to G 2 of V 2 looking for

100
00:05:01.210 --> 00:05:04.870
now if we assume you're doing binary

101
00:05:02.740 --> 00:05:06.610
classification then this activation

102
00:05:04.870 --> 00:05:08.560
function really should be the sigmoid

103
00:05:06.610 --> 00:05:11.080
function so I'm just throw that in here

104
00:05:08.560 --> 00:05:13.870
so that's the forward propagation or the

105
00:05:11.080 --> 00:05:15.729
left-to-right forward computation for

106
00:05:13.870 --> 00:05:18.430
your neural network next let's compute

107
00:05:15.729 --> 00:05:24.750
the derivatives so this is the back

108
00:05:18.430 --> 00:05:30.750
propagation step then it computes DZ 2

109
00:05:24.750 --> 00:05:33.610
equals a 2 minus the ground truth Y and

110
00:05:30.750 --> 00:05:36.580
just just as a reminder all this is

111
00:05:33.610 --> 00:05:41.289
vectorize across example so the matrix Y

112
00:05:36.580 --> 00:05:45.280
is this sum 1 by M matrix that lists all

113
00:05:41.289 --> 00:05:51.370
of your M examples horizontally then it

114
00:05:45.280 --> 00:05:55.330
turns out DW 2 is equal to this in fact

115
00:05:51.370 --> 00:05:58.870
um these first three equations are very

116
00:05:55.330 --> 00:06:00.900
similar to gradient descent for logistic

117
00:05:58.870 --> 00:06:00.900
regression

118
00:06:00.960 --> 00:06:12.610
X is equals 1 comma um keep DIMMs equals

119
00:06:07.419 --> 00:06:15.580
true and just a little detail this NP

120
00:06:12.610 --> 00:06:18.070
dot some is a Python numpy commands or

121
00:06:15.580 --> 00:06:21.100
something across one dimension of a

122
00:06:18.070 --> 00:06:24.810
matrix in this case summing horizontally

123
00:06:21.100 --> 00:06:27.600
and what q DIMMs does is it prevents

124
00:06:24.810 --> 00:06:31.230
- from outputting one of those funny

125
00:06:27.600 --> 00:06:34.650
bank 1 various red where the dimensions

126
00:06:31.230 --> 00:06:37.010
was you know n comma so by having keep

127
00:06:34.650 --> 00:06:41.280
them sequels true this ensures that

128
00:06:37.010 --> 00:06:44.580
Python outputs 4gb to a vector that is

129
00:06:41.280 --> 00:06:47.820
sum and buy one technically this will be

130
00:06:44.580 --> 00:06:50.130
I guess n to buy one in this case is

131
00:06:47.820 --> 00:06:53.520
just a one by one number so maybe it

132
00:06:50.130 --> 00:06:56.790
doesn't matter but later on we'll see

133
00:06:53.520 --> 00:06:58.500
when it really matters so so far what

134
00:06:56.790 --> 00:07:01.320
we've done is very similar to logistic

135
00:06:58.500 --> 00:07:03.919
regression but now as you compute

136
00:07:01.320 --> 00:07:14.370
continue to run back propagation you

137
00:07:03.919 --> 00:07:19.380
would compute this Z 2 times G 1 prime

138
00:07:14.370 --> 00:07:20.880
of Z 1 so this quantity G 1 prime is the

139
00:07:19.380 --> 00:07:22.919
derivative of whatever was the

140
00:07:20.880 --> 00:07:25.770
activation function you use for the

141
00:07:22.919 --> 00:07:27.030
hidden layer and for the output layer I

142
00:07:25.770 --> 00:07:29.400
assume that you're doing binary

143
00:07:27.030 --> 00:07:30.780
classification with the sigmoid function

144
00:07:29.400 --> 00:07:34.620
so that's already baked into that

145
00:07:30.780 --> 00:07:39.090
formula for DZ 2 and this times is a

146
00:07:34.620 --> 00:07:43.050
element-wise product so this year

147
00:07:39.090 --> 00:07:46.950
there's going to be an N 1 by M matrix

148
00:07:43.050 --> 00:07:48.990
and this here this element wise

149
00:07:46.950 --> 00:07:52.680
derivative thing is also going to be an

150
00:07:48.990 --> 00:07:54.720
N 1 by n matrix and so this times there

151
00:07:52.680 --> 00:07:59.669
is another min wise product of two

152
00:07:54.720 --> 00:08:08.490
matrices then finally DW 1 is equal to

153
00:07:59.669 --> 00:08:18.950
that and DV 1 is equal to this and P dot

154
00:08:08.490 --> 00:08:21.900
some D Z 1 X is equals 1 keep Tim's

155
00:08:18.950 --> 00:08:23.430
equals true so whereas

156
00:08:21.900 --> 00:08:27.210
previously the cheap business maybe

157
00:08:23.430 --> 00:08:28.590
matter less if n2 is equal to 1000 sofa

158
00:08:27.210 --> 00:08:35.729
one by one thing is there's a real

159
00:08:28.590 --> 00:08:38.370
number here P p1 will be a n 1 by 1

160
00:08:35.729 --> 00:08:40.110
vector and so you want Python you want n

161
00:08:38.370 --> 00:08:43.110
P dot some output something of this

162
00:08:40.110 --> 00:08:46.529
dimension rather than a family Matic one

163
00:08:43.110 --> 00:08:48.360
array of that dimension which could end

164
00:08:46.529 --> 00:08:50.580
up messing up some of your later

165
00:08:48.360 --> 00:08:53.310
calculations the other way would be to

166
00:08:50.580 --> 00:08:56.910
not have to keep them parameters but to

167
00:08:53.310 --> 00:09:00.060
explicitly call in a reshape to reshape

168
00:08:56.910 --> 00:09:04.400
the output of NP dot some into this

169
00:09:00.060 --> 00:09:08.310
dimension which you would like DB so how

170
00:09:04.400 --> 00:09:11.339
so that was for propagation in I guess

171
00:09:08.310 --> 00:09:14.310
four equations and back propagation in I

172
00:09:11.339 --> 00:09:16.680
guess six equations I know I just wrote

173
00:09:14.310 --> 00:09:18.870
down these equations but in the next

174
00:09:16.680 --> 00:09:22.050
optional video let's go over some

175
00:09:18.870 --> 00:09:23.940
intuitions for how the six equations for

176
00:09:22.050 --> 00:09:25.830
the back propagation algorithm were

177
00:09:23.940 --> 00:09:27.750
derived please feel free to watch that

178
00:09:25.830 --> 00:09:30.000
or not but either way if you implement

179
00:09:27.750 --> 00:09:32.730
these algorithms you will have a correct

180
00:09:30.000 --> 00:09:34.650
implementation of for profit backdrop

181
00:09:32.730 --> 00:09:36.750
and you'll be able to compute the

182
00:09:34.650 --> 00:09:39.029
derivatives you need in order to apply

183
00:09:36.750 --> 00:09:41.520
gradient descent to learn the parameters

184
00:09:39.029 --> 00:09:43.680
of your neural network it is possible to

185
00:09:41.520 --> 00:09:45.209
implement design room and get it to work

186
00:09:43.680 --> 00:09:47.130
without deeply understanding the

187
00:09:45.209 --> 00:09:50.520
calculus a lot of successful people

188
00:09:47.130 --> 00:09:52.320
earning practitioners do so but if you

189
00:09:50.520 --> 00:09:54.180
want you can also watch the next video

190
00:09:52.320 --> 00:09:56.580
just to get a bit more intuition about

191
00:09:54.180 --> 00:09:58.820
the derivation of these of these

192
00:09:56.580 --> 00:09:58.820
equations