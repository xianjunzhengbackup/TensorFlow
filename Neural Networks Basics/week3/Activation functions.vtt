WEBVTT

1
00:00:00.390 --> 00:00:04.350
when you boost a neural network one of

2
00:00:02.580 --> 00:00:06.720
the choices you get to make is what

3
00:00:04.350 --> 00:00:09.599
activation functions use independent

4
00:00:06.720 --> 00:00:11.490
layers as well as at the output unit of

5
00:00:09.599 --> 00:00:13.139
your neural network so far we've just

6
00:00:11.490 --> 00:00:16.080
been using the sigmoid activation

7
00:00:13.139 --> 00:00:18.720
function but sometimes other choices can

8
00:00:16.080 --> 00:00:20.939
work much better let's take a look at

9
00:00:18.720 --> 00:00:23.279
some of the options in the forward

10
00:00:20.939 --> 00:00:26.099
propagation steps for the neural network

11
00:00:23.279 --> 00:00:28.710
we have these two steps where we use the

12
00:00:26.099 --> 00:00:32.610
sigmoid function here so that sigmoid is

13
00:00:28.710 --> 00:00:37.590
called an activation function and G is

14
00:00:32.610 --> 00:00:40.680
the familiar sigmoid function I equals 1

15
00:00:37.590 --> 00:00:42.600
over 1 plus nu to negative G so in the

16
00:00:40.680 --> 00:00:49.739
more general case we can have a

17
00:00:42.600 --> 00:00:53.309
different function G of Z visually right

18
00:00:49.739 --> 00:00:56.010
here where G could be a nonlinear

19
00:00:53.309 --> 00:00:59.250
function that may not be the sigmoid

20
00:00:56.010 --> 00:01:01.879
function so for example the sigmoid

21
00:00:59.250 --> 00:01:04.290
function goes between 0 & 1 an

22
00:01:01.879 --> 00:01:06.900
activation function that almost always

23
00:01:04.290 --> 00:01:10.320
works better than the sigmoid function

24
00:01:06.900 --> 00:01:14.189
is the 10h function or the hyperbolic

25
00:01:10.320 --> 00:01:19.979
tangent function so this is Z this is a

26
00:01:14.189 --> 00:01:25.710
this is a equals 10 H of Z and this goes

27
00:01:19.979 --> 00:01:31.079
between plus 1 and minus 1 the formula

28
00:01:25.710 --> 00:01:37.799
for the 10h function is e to the Z minus

29
00:01:31.079 --> 00:01:40.140
e to negative V over there some and it's

30
00:01:37.799 --> 00:01:43.890
actually mathematically a shifted

31
00:01:40.140 --> 00:01:46.350
version of the sigmoid function so as a

32
00:01:43.890 --> 00:01:49.860
you know sigmoid function just like that

33
00:01:46.350 --> 00:01:52.079
but shift it so that it now crosses a

34
00:01:49.860 --> 00:01:54.570
zero zero point and rescale so it goes

35
00:01:52.079 --> 00:01:58.530
to G minus one and plus one and it turns

36
00:01:54.570 --> 00:02:05.340
out that for hidden units if you let the

37
00:01:58.530 --> 00:02:09.910
function G of Z be equal to

38
00:02:05.340 --> 00:02:12.490
ten HSV this almost always works better

39
00:02:09.910 --> 00:02:14.020
than the sigmoid function because with

40
00:02:12.490 --> 00:02:16.930
values between plus one and minus one

41
00:02:14.020 --> 00:02:19.000
the mean of the activations that come

42
00:02:16.930 --> 00:02:21.550
out of your head in there are closer to

43
00:02:19.000 --> 00:02:23.020
having a zero mean and so just as

44
00:02:21.550 --> 00:02:23.590
sometimes when you train a learning

45
00:02:23.020 --> 00:02:25.690
algorithm

46
00:02:23.590 --> 00:02:29.709
you might Center the data and have your

47
00:02:25.690 --> 00:02:31.510
data have zero mean using a 10-8 instead

48
00:02:29.709 --> 00:02:34.750
of a sigmoid function kind of has the

49
00:02:31.510 --> 00:02:36.880
effect of centering your data so that

50
00:02:34.750 --> 00:02:39.610
the mean of the data is close to the

51
00:02:36.880 --> 00:02:41.410
zero rather than maybe a 0.5 and this

52
00:02:39.610 --> 00:02:43.510
actually makes learning for the next

53
00:02:41.410 --> 00:02:45.820
layer a little bit easier we'll say more

54
00:02:43.510 --> 00:02:47.380
about this in the second course when we

55
00:02:45.820 --> 00:02:50.739
talk about optimization algorithms as

56
00:02:47.380 --> 00:02:52.480
well but one takeaway is that I pretty

57
00:02:50.739 --> 00:02:54.250
much never use the sigmoid activation

58
00:02:52.480 --> 00:02:56.410
function anymore

59
00:02:54.250 --> 00:02:59.560
the 10-ish function is almost always

60
00:02:56.410 --> 00:03:03.550
strictly superior the one exception is

61
00:02:59.560 --> 00:03:07.420
for the output layer because if Y is

62
00:03:03.550 --> 00:03:10.570
either 0 or 1 then it makes sense for y

63
00:03:07.420 --> 00:03:13.989
hat to be a number that you want to

64
00:03:10.570 --> 00:03:16.570
output plus between 0 and 1 rather than

65
00:03:13.989 --> 00:03:19.360
between minus 1 and 1 so the one

66
00:03:16.570 --> 00:03:21.430
exception where I would use the sigmoid

67
00:03:19.360 --> 00:03:24.670
activation function is when you're using

68
00:03:21.430 --> 00:03:26.350
binary classification in which case you

69
00:03:24.670 --> 00:03:29.709
might use the sigmoid activation

70
00:03:26.350 --> 00:03:35.170
function for the output layer so G of Z

71
00:03:29.709 --> 00:03:37.180
2 here is equal to Sigma of Z 2 and so

72
00:03:35.170 --> 00:03:40.299
what you see in this example is where

73
00:03:37.180 --> 00:03:43.920
you might have a ten-inch activation

74
00:03:40.299 --> 00:03:47.769
function for the hidden layer and

75
00:03:43.920 --> 00:03:49.299
sigmoid for the output layer so the

76
00:03:47.769 --> 00:03:51.670
activation functions can be different

77
00:03:49.299 --> 00:03:53.709
for different layers and sometimes to

78
00:03:51.670 --> 00:03:55.690
denote that the activation functions are

79
00:03:53.709 --> 00:03:58.510
different for different layers we might

80
00:03:55.690 --> 00:04:02.230
use these square brackets super scripts

81
00:03:58.510 --> 00:04:04.540
as well to indicate that G of square

82
00:04:02.230 --> 00:04:06.940
bracket 1 may be different than G square

83
00:04:04.540 --> 00:04:09.340
bracket 2 red mccain square bracket 1

84
00:04:06.940 --> 00:04:11.470
superscript refers to this layer and

85
00:04:09.340 --> 00:04:12.879
superscript square bracket 2 refers to

86
00:04:11.470 --> 00:04:15.680
the Alpha layer

87
00:04:12.879 --> 00:04:18.109
now one of the downsides of both the

88
00:04:15.680 --> 00:04:20.780
sigmoid function and the 10h function is

89
00:04:18.109 --> 00:04:22.910
that if Z is either very large or very

90
00:04:20.780 --> 00:04:24.460
small then the gradient of the

91
00:04:22.910 --> 00:04:27.560
derivative or the slope of this function

92
00:04:24.460 --> 00:04:30.139
becomes very small so Z is very large or

93
00:04:27.560 --> 00:04:33.169
Z is very small the slope of the

94
00:04:30.139 --> 00:04:35.270
function you know ends up being close to

95
00:04:33.169 --> 00:04:38.360
zero and so this can slow down gradient

96
00:04:35.270 --> 00:04:41.810
descent so one of the toys that is very

97
00:04:38.360 --> 00:04:44.900
popular in machine learning is what's

98
00:04:41.810 --> 00:04:50.720
called the rectified linear unit so the

99
00:04:44.900 --> 00:04:57.110
value function looks like this and the

100
00:04:50.720 --> 00:05:00.500
formula is a equals max of 0 comma Z so

101
00:04:57.110 --> 00:05:03.530
the derivative is 1 so long as Z is

102
00:05:00.500 --> 00:05:05.990
positive and derivative or the slope is

103
00:05:03.530 --> 00:05:07.580
0 when Z is negative if you're

104
00:05:05.990 --> 00:05:10.190
implementing this technically the

105
00:05:07.580 --> 00:05:12.349
derivative when Z is exactly 0 is not

106
00:05:10.190 --> 00:05:14.210
well-defined but when you implement is

107
00:05:12.349 --> 00:05:18.770
in the computer the often you get

108
00:05:14.210 --> 00:05:21.229
exactly the equals 0 0 0 0 0 0 0 0 0 0

109
00:05:18.770 --> 00:05:22.940
it is very small so you don't need to

110
00:05:21.229 --> 00:05:25.610
worry about it in practice you could

111
00:05:22.940 --> 00:05:29.659
pretend a derivative when Z is equal to

112
00:05:25.610 --> 00:05:32.270
0 you can pretend is either 1 or 0 and

113
00:05:29.659 --> 00:05:35.479
you can work just fine so the fact that

114
00:05:32.270 --> 00:05:37.430
is not differentiable the fact that so

115
00:05:35.479 --> 00:05:40.010
here are some rules of thumb for

116
00:05:37.430 --> 00:05:43.280
choosing activation functions if your

117
00:05:40.010 --> 00:05:45.620
output is 0 1 value if you're I'm using

118
00:05:43.280 --> 00:05:47.539
binary classification then the sigmoid

119
00:05:45.620 --> 00:05:50.479
activation function is very natural for

120
00:05:47.539 --> 00:05:59.419
the upper layer and then for all other

121
00:05:50.479 --> 00:06:04.460
units on varalu or the rectified linear

122
00:05:59.419 --> 00:06:07.190
unit is increasingly the default choice

123
00:06:04.460 --> 00:06:10.280
of activation function so if you're not

124
00:06:07.190 --> 00:06:13.849
sure what to use for your head in there

125
00:06:10.280 --> 00:06:15.289
I would just use the relu activation

126
00:06:13.849 --> 00:06:17.570
function that's what you see most people

127
00:06:15.289 --> 00:06:20.120
using these days although sometimes

128
00:06:17.570 --> 00:06:21.350
people also use the tannish activation

129
00:06:20.120 --> 00:06:23.150
function

130
00:06:21.350 --> 00:06:26.270
once this advantage of the value is that

131
00:06:23.150 --> 00:06:28.640
the derivative is equal to zero when V

132
00:06:26.270 --> 00:06:31.700
is negative in practice this works just

133
00:06:28.640 --> 00:06:33.890
fine but there is another version of the

134
00:06:31.700 --> 00:06:35.420
value called the least G value will give

135
00:06:33.890 --> 00:06:38.690
you the formula on the next slide but

136
00:06:35.420 --> 00:06:40.520
instead of it being zero when G is

137
00:06:38.690 --> 00:06:42.940
negative it just takes a slight slope

138
00:06:40.520 --> 00:06:47.900
like so so this is called the Whiskey

139
00:06:42.940 --> 00:06:51.170
value this usually works better than the

140
00:06:47.900 --> 00:06:53.900
value activation function although it's

141
00:06:51.170 --> 00:06:54.860
just not used as much in practice either

142
00:06:53.900 --> 00:06:56.770
one should be fine

143
00:06:54.860 --> 00:06:59.330
although if you had to pick one I

144
00:06:56.770 --> 00:07:01.460
usually just use the revenue and the

145
00:06:59.330 --> 00:07:04.460
advantage of both the value and only key

146
00:07:01.460 --> 00:07:06.500
value is that for a lot of the space of

147
00:07:04.460 --> 00:07:08.150
Z the derivative of the activation

148
00:07:06.500 --> 00:07:11.870
function the slope of the activation

149
00:07:08.150 --> 00:07:13.970
function is very different from zero and

150
00:07:11.870 --> 00:07:15.920
so in practice using the regular

151
00:07:13.970 --> 00:07:18.590
activation function your new network

152
00:07:15.920 --> 00:07:20.810
will often learn much faster than using

153
00:07:18.590 --> 00:07:23.840
the ten age or the sigmoid activation

154
00:07:20.810 --> 00:07:26.420
function and the main reason is that on

155
00:07:23.840 --> 00:07:28.700
this less of this effect of the slope of

156
00:07:26.420 --> 00:07:31.580
the function going to zero which slows

157
00:07:28.700 --> 00:07:33.950
down learning and I know that for half

158
00:07:31.580 --> 00:07:36.710
of the range of Z the slope of relu is

159
00:07:33.950 --> 00:07:39.050
zero but in practice enough of your

160
00:07:36.710 --> 00:07:41.120
hidden units will have Z greater than

161
00:07:39.050 --> 00:07:43.700
zero so learning can still be quite mask

162
00:07:41.120 --> 00:07:45.800
for most training examples so let's just

163
00:07:43.700 --> 00:07:47.600
quickly recap there are pros and cons of

164
00:07:45.800 --> 00:07:50.030
different activation functions here's

165
00:07:47.600 --> 00:07:52.790
the sigmoid activation function I would

166
00:07:50.030 --> 00:07:54.410
say never use this except for the output

167
00:07:52.790 --> 00:07:56.330
layer if you are doing binary

168
00:07:54.410 --> 00:07:59.540
classification or maybe almost never use

169
00:07:56.330 --> 00:08:02.720
this and the reason I almost never use

170
00:07:59.540 --> 00:08:05.060
this is because the 10h is pretty much

171
00:08:02.720 --> 00:08:12.080
strictly superior so the 10-inch

172
00:08:05.060 --> 00:08:13.430
activation function is this and then the

173
00:08:12.080 --> 00:08:15.490
default the most commonly used

174
00:08:13.430 --> 00:08:19.100
activation function is the Grandview

175
00:08:15.490 --> 00:08:23.660
which is this so you're not sure what

176
00:08:19.100 --> 00:08:26.600
else to use use this one and maybe you

177
00:08:23.660 --> 00:08:31.930
know feel free also to try to leek you

178
00:08:26.600 --> 00:08:36.659
really know where um might be

179
00:08:31.930 --> 00:08:40.390
0.01 G Komen Z right so a is the max of

180
00:08:36.659 --> 00:08:43.810
0.01 times Z and Z so that gives you

181
00:08:40.390 --> 00:08:46.200
this some Bend in the function and you

182
00:08:43.810 --> 00:08:51.670
might say you know why is that constant

183
00:08:46.200 --> 00:08:53.380
0.01 well you can also make that another

184
00:08:51.670 --> 00:08:54.670
parameter of the learning algorithm and

185
00:08:53.380 --> 00:08:58.480
some people say that works even better

186
00:08:54.670 --> 00:08:59.649
but I hardly see people do that so but

187
00:08:58.480 --> 00:09:01.360
if you feel like trying it in your

188
00:08:59.649 --> 00:09:03.430
application you know please feel free to

189
00:09:01.360 --> 00:09:05.800
do so and and you can just see how it

190
00:09:03.430 --> 00:09:08.290
works and how long works and stick with

191
00:09:05.800 --> 00:09:09.880
it if it gives you good result so I hope

192
00:09:08.290 --> 00:09:11.620
that gives you a sense of some of the

193
00:09:09.880 --> 00:09:13.870
choices of activation functions you can

194
00:09:11.620 --> 00:09:15.940
use in your network one of the themes

195
00:09:13.870 --> 00:09:18.130
we'll see in deep learning is that you

196
00:09:15.940 --> 00:09:20.110
often have a lot of different choices in

197
00:09:18.130 --> 00:09:22.089
how you code your neural network ranging

198
00:09:20.110 --> 00:09:24.430
from number of credit units to the

199
00:09:22.089 --> 00:09:25.839
chosen activation function to how you

200
00:09:24.430 --> 00:09:28.480
neutralize the waves which we'll see

201
00:09:25.839 --> 00:09:30.880
later a lot of choices like that and it

202
00:09:28.480 --> 00:09:33.279
turns out that is sometimes difficult to

203
00:09:30.880 --> 00:09:35.649
get good guidelines for exactly what

204
00:09:33.279 --> 00:09:37.270
will work best for your problem so so

205
00:09:35.649 --> 00:09:39.070
these three causes I'll keep on giving

206
00:09:37.270 --> 00:09:40.839
you a sense of what I see in the

207
00:09:39.070 --> 00:09:43.450
industry in terms of what's more or less

208
00:09:40.839 --> 00:09:45.520
popular but for your application with

209
00:09:43.450 --> 00:09:46.930
your applications video synthesis it's

210
00:09:45.520 --> 00:09:49.450
actually very difficult to know in

211
00:09:46.930 --> 00:09:51.400
advance exactly what will work best so

212
00:09:49.450 --> 00:09:52.930
concrete values would be if you're not

213
00:09:51.400 --> 00:09:54.940
sure which one of these activation

214
00:09:52.930 --> 00:09:57.700
functions work best you know try them

215
00:09:54.940 --> 00:10:00.010
all and then evaluate on like a holdout

216
00:09:57.700 --> 00:10:02.529
validation set or like a development set

217
00:10:00.010 --> 00:10:04.480
which we'll talk about later and see

218
00:10:02.529 --> 00:10:08.350
which one works better and then go of

219
00:10:04.480 --> 00:10:10.180
that and I think that by testing these

220
00:10:08.350 --> 00:10:13.510
different choices for your application

221
00:10:10.180 --> 00:10:16.240
you'd be better at future proofing your

222
00:10:13.510 --> 00:10:18.130
neural network architecture against the

223
00:10:16.240 --> 00:10:20.550
the distinction sees our problem as well

224
00:10:18.130 --> 00:10:23.440
evolutions of the algorithms rather than

225
00:10:20.550 --> 00:10:25.630
you know if I were to tell you always

226
00:10:23.440 --> 00:10:27.339
use a random activation and don't use

227
00:10:25.630 --> 00:10:29.440
anything else that that just may or may

228
00:10:27.339 --> 00:10:30.790
not apply for whatever problem you end

229
00:10:29.440 --> 00:10:32.410
up working on you know either

230
00:10:30.790 --> 00:10:36.220
either in the near future on the distant

231
00:10:32.410 --> 00:10:37.870
future all right so that was a choice of

232
00:10:36.220 --> 00:10:39.310
activation functions you've seen the

233
00:10:37.870 --> 00:10:41.459
most popular activation functions

234
00:10:39.310 --> 00:10:44.260
there's one other question that

235
00:10:41.459 --> 00:10:45.160
sometimes is ask which is why do you

236
00:10:44.260 --> 00:10:46.959
even need to you

237
00:10:45.160 --> 00:10:49.240
activation function at all why not just

238
00:10:46.959 --> 00:10:49.779
do away with that so let's talk about

239
00:10:49.240 --> 00:10:52.240
that

240
00:10:49.779 --> 00:10:54.430
in the next video and when you see why

241
00:10:52.240 --> 00:10:58.259
new network do means some sort of

242
00:10:54.430 --> 00:10:58.259
nonlinear activation function