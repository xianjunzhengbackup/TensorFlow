WEBVTT

1
00:00:00.030 --> 00:00:04.440
in the last video you saw what a single

2
00:00:02.370 --> 00:00:06.270
hidden layer neural network looks like

3
00:00:04.440 --> 00:00:08.309
in this video let's go through the

4
00:00:06.270 --> 00:00:10.710
details of exactly how this neural

5
00:00:08.309 --> 00:00:13.860
network computers outputs what you see

6
00:00:10.710 --> 00:00:15.960
is that is like logistic regression the

7
00:00:13.860 --> 00:00:18.390
repeater of all the times let's take a

8
00:00:15.960 --> 00:00:19.350
look so this is what's a two layer

9
00:00:18.390 --> 00:00:22.109
neural network

10
00:00:19.350 --> 00:00:23.130
let's go more deeply into exactly what

11
00:00:22.109 --> 00:00:25.380
this neural network

12
00:00:23.130 --> 00:00:28.140
compute now was said before that

13
00:00:25.380 --> 00:00:30.000
logistic regression the circle images

14
00:00:28.140 --> 00:00:32.189
the regression really represents two

15
00:00:30.000 --> 00:00:34.380
steps of computation first you compute Z

16
00:00:32.189 --> 00:00:37.440
as follows and in second you compute the

17
00:00:34.380 --> 00:00:39.870
activation as a sigmoid function of Z so

18
00:00:37.440 --> 00:00:42.090
a neural network just does this a lot

19
00:00:39.870 --> 00:00:43.890
more times let's start by focusing on

20
00:00:42.090 --> 00:00:45.629
just one of the nodes in the hidden

21
00:00:43.890 --> 00:00:47.160
there and this look at the first node in

22
00:00:45.629 --> 00:00:48.870
the hidden layer so I've grayed out the

23
00:00:47.160 --> 00:00:50.820
other nodes for now so similar to

24
00:00:48.870 --> 00:00:53.489
logistic regression on the left this

25
00:00:50.820 --> 00:00:55.379
node in a hidden layer does two steps of

26
00:00:53.489 --> 00:00:57.629
computation right the first step and

27
00:00:55.379 --> 00:01:01.649
think it's as the left half of this node

28
00:00:57.629 --> 00:01:05.339
it computes Z equals W transpose X plus

29
00:01:01.649 --> 00:01:07.290
B and the notation we'll use is um these

30
00:01:05.339 --> 00:01:09.030
are all quantities associated with the

31
00:01:07.290 --> 00:01:10.619
first hidden layer so that's why we have

32
00:01:09.030 --> 00:01:13.170
a bunch of square brackets there and

33
00:01:10.619 --> 00:01:14.850
this is the first node in the hidden

34
00:01:13.170 --> 00:01:17.310
layer so that's why we have the

35
00:01:14.850 --> 00:01:19.409
subscript one over there so first it

36
00:01:17.310 --> 00:01:23.400
does that and then a second step is it

37
00:01:19.409 --> 00:01:26.100
computes a 1 1 equals sigmoid of z11

38
00:01:23.400 --> 00:01:29.970
like so so for both Z and ay the

39
00:01:26.100 --> 00:01:32.159
notational convention is that a Li the L

40
00:01:29.970 --> 00:01:33.840
here in superscript square brackets

41
00:01:32.159 --> 00:01:36.960
refers to layer number and the I

42
00:01:33.840 --> 00:01:38.820
subscript here refers to the nodes in

43
00:01:36.960 --> 00:01:40.890
that layer so then they will be looking

44
00:01:38.820 --> 00:01:43.560
at is layer 1 that is a hidden layer

45
00:01:40.890 --> 00:01:46.439
node 1 so that's why the superscript and

46
00:01:43.560 --> 00:01:48.689
subscript were on both 1 1 so that

47
00:01:46.439 --> 00:01:51.149
little circle that first node in your

48
00:01:48.689 --> 00:01:53.220
network represents carrying out these

49
00:01:51.149 --> 00:01:55.320
two steps of computation now let's look

50
00:01:53.220 --> 00:01:57.299
at the second node in your network the

51
00:01:55.320 --> 00:01:59.790
second node in the hidden layer of in

52
00:01:57.299 --> 00:02:01.979
your network similar to the logistic

53
00:01:59.790 --> 00:02:03.930
regression unit on the left this little

54
00:02:01.979 --> 00:02:06.030
circle represents two steps of

55
00:02:03.930 --> 00:02:08.520
computation the first step is a

56
00:02:06.030 --> 00:02:10.920
confusing Z this is still layer 1

57
00:02:08.520 --> 00:02:13.470
pronounced the second node equals W

58
00:02:10.920 --> 00:02:18.930
transpose X plus V

59
00:02:13.470 --> 00:02:20.640
- and then a 1/2 equals Sigma z12 and

60
00:02:18.930 --> 00:02:22.170
again feel free to pause the video if

61
00:02:20.640 --> 00:02:24.360
you want but you can double check that

62
00:02:22.170 --> 00:02:26.730
the superscript and subscript notation

63
00:02:24.360 --> 00:02:28.980
is consistent with what we have written

64
00:02:26.730 --> 00:02:31.650
here above in purple so we'll talk

65
00:02:28.980 --> 00:02:33.870
through the first two hidden units in

66
00:02:31.650 --> 00:02:35.700
the neural network on hidden units three

67
00:02:33.870 --> 00:02:38.640
and four also represents some

68
00:02:35.700 --> 00:02:41.580
computations so now let me take this

69
00:02:38.640 --> 00:02:43.410
pair of equations and this pair of

70
00:02:41.580 --> 00:02:45.420
equations and let's copy them to the

71
00:02:43.410 --> 00:02:47.880
moon fly so here's our network and

72
00:02:45.420 --> 00:02:49.920
here's the first and there's a second

73
00:02:47.880 --> 00:02:53.400
equations they were worked on previously

74
00:02:49.920 --> 00:02:55.560
for the first and the second hidden

75
00:02:53.400 --> 00:02:57.840
units if you then go through and write

76
00:02:55.560 --> 00:03:00.360
out the corresponding equations for the

77
00:02:57.840 --> 00:03:02.460
third and fourth hidden units you get

78
00:03:00.360 --> 00:03:06.150
the following and those make sure this

79
00:03:02.460 --> 00:03:09.450
notation is clear this is the vector W 1

80
00:03:06.150 --> 00:03:11.070
1 this is a vector transpose x I think

81
00:03:09.450 --> 00:03:13.140
so that's what the superscript G there

82
00:03:11.070 --> 00:03:15.480
represents this is a vector transpose

83
00:03:13.140 --> 00:03:16.920
now as you might have guessed if you're

84
00:03:15.480 --> 00:03:19.410
actually implementing in your network

85
00:03:16.920 --> 00:03:21.630
doing this with a for loop seems really

86
00:03:19.410 --> 00:03:24.989
inefficient so what we're going to do is

87
00:03:21.630 --> 00:03:26.700
take these four equations and vectorize

88
00:03:24.989 --> 00:03:29.760
so I'm going to start by showing how to

89
00:03:26.700 --> 00:03:30.690
compute Z as a vector it turns out you

90
00:03:29.760 --> 00:03:34.080
could do it as follows

91
00:03:30.690 --> 00:03:37.739
let me take these WS and stack them into

92
00:03:34.080 --> 00:03:40.140
a matrix then you have W 1 1 transpose

93
00:03:37.739 --> 00:03:41.670
so that's a row vector of the column

94
00:03:40.140 --> 00:03:47.100
vector transpose gives you a row vector

95
00:03:41.670 --> 00:03:50.160
then W 1 2 transpose W 1 3 transpose of

96
00:03:47.100 --> 00:03:53.340
V 1 4 transpose and so this by stacking

97
00:03:50.160 --> 00:03:55.260
goes from for W vectors together you end

98
00:03:53.340 --> 00:03:57.420
up with a matrix so another way to think

99
00:03:55.260 --> 00:03:59.670
of this is that we have for logistic

100
00:03:57.420 --> 00:04:01.440
regression unions there and each of the

101
00:03:59.670 --> 00:04:04.140
logistic regression unions have a

102
00:04:01.440 --> 00:04:06.150
corresponding parameter vector W and by

103
00:04:04.140 --> 00:04:09.180
stacking those four vectors together you

104
00:04:06.150 --> 00:04:11.310
end up with this four by three matrix so

105
00:04:09.180 --> 00:04:14.160
if you then take this matrix and

106
00:04:11.310 --> 00:04:17.700
multiply it by your input features x1 x2

107
00:04:14.160 --> 00:04:19.680
x3 you end up with by our matrix

108
00:04:17.700 --> 00:04:24.990
multiplication works you end up with w1

109
00:04:19.680 --> 00:04:26.260
1 transpose x w1 w2 1 transpose X of U 3

110
00:04:24.990 --> 00:04:29.800
1 transyl

111
00:04:26.260 --> 00:04:32.620
XW 1 transpose X and then let's not

112
00:04:29.800 --> 00:04:33.340
forget the bees so we now add to this a

113
00:04:32.620 --> 00:04:40.450
vector

114
00:04:33.340 --> 00:04:46.630
b11 b12 b13 in 1/4 so that they see this

115
00:04:40.450 --> 00:04:48.880
then this is b11 b12 b13 e 1/4 and so

116
00:04:46.630 --> 00:04:51.520
you see that each of the four rows of

117
00:04:48.880 --> 00:04:53.860
this outcome correspond exactly to each

118
00:04:51.520 --> 00:04:56.260
of these four rows of each these four

119
00:04:53.860 --> 00:04:58.090
quantities that we had above so in other

120
00:04:56.260 --> 00:05:02.890
words we've just shown that this thing

121
00:04:58.090 --> 00:05:05.950
is therefore equal to V 1 1 V 1 to V 1 V

122
00:05:02.890 --> 00:05:07.630
V 1 4 right as defined here and maybe

123
00:05:05.950 --> 00:05:10.060
not surprisingly we're going to call

124
00:05:07.630 --> 00:05:11.890
this whole thing the vector V 1 which is

125
00:05:10.060 --> 00:05:14.560
taken by stacking up these um

126
00:05:11.890 --> 00:05:16.810
individuals of these into a column

127
00:05:14.560 --> 00:05:19.360
vector when we're vectorizing one of the

128
00:05:16.810 --> 00:05:21.070
rules of thumb that might help you

129
00:05:19.360 --> 00:05:23.140
navigate this is that when we have

130
00:05:21.070 --> 00:05:25.420
different nodes in a layer or stack them

131
00:05:23.140 --> 00:05:28.390
vertically so that's why when you have Z

132
00:05:25.420 --> 00:05:30.460
1 1 2 Z 1 for those correspond to four

133
00:05:28.390 --> 00:05:32.590
different nodes in the hidden layer and

134
00:05:30.460 --> 00:05:34.920
so we stack these four numbers

135
00:05:32.590 --> 00:05:37.810
vertically to form the vectors V 1 and

136
00:05:34.920 --> 00:05:40.150
reduce one more piece of notation this 4

137
00:05:37.810 --> 00:05:43.840
by 3 matrix here which we obtained by

138
00:05:40.150 --> 00:05:45.460
stacking the lower case you know W 1 1 W

139
00:05:43.840 --> 00:05:49.060
1 2 and so on we're going to call this

140
00:05:45.460 --> 00:05:51.760
matrix W capital 1 and similarly this

141
00:05:49.060 --> 00:05:53.890
vector or going to call B superscript 1

142
00:05:51.760 --> 00:05:56.740
square bracket and so this is a 4 by 1

143
00:05:53.890 --> 00:05:59.920
vector so now we've computed Z using

144
00:05:56.740 --> 00:06:01.750
this vector matrix notation the last

145
00:05:59.920 --> 00:06:04.240
thing we need to do is also compute

146
00:06:01.750 --> 00:06:06.340
these values of a and so probably won't

147
00:06:04.240 --> 00:06:09.400
surprise you to see that we're going to

148
00:06:06.340 --> 00:06:13.240
define a 1 as just stacking together

149
00:06:09.400 --> 00:06:15.190
those activation values a11 to a14 so

150
00:06:13.240 --> 00:06:18.310
just take these 4 values and stack them

151
00:06:15.190 --> 00:06:21.190
together in a vector called a1 and this

152
00:06:18.310 --> 00:06:23.380
is going to be sigmoid of z1 where

153
00:06:21.190 --> 00:06:25.750
there's no husband implementation of the

154
00:06:23.380 --> 00:06:27.940
sigmoid function that takes in the four

155
00:06:25.750 --> 00:06:31.180
elements of Z and applies the sigmoid

156
00:06:27.940 --> 00:06:35.020
function element wise to it so just a

157
00:06:31.180 --> 00:06:38.230
recap we figured out that z1 is equal to

158
00:06:35.020 --> 00:06:39.559
W 1 times the vector X plus the vector B

159
00:06:38.230 --> 00:06:43.069
1 and a 1

160
00:06:39.559 --> 00:06:44.779
is sigmoid x z 1 let's just copy this to

161
00:06:43.069 --> 00:06:46.429
the next slide and what we see is that

162
00:06:44.779 --> 00:06:49.219
for the first layer of the neural

163
00:06:46.429 --> 00:06:52.609
network given an input X we have that z1

164
00:06:49.219 --> 00:06:56.329
is equal to w1 times X plus B 1 and a 1

165
00:06:52.609 --> 00:06:59.899
is sick point of z1 and the dimensions

166
00:06:56.329 --> 00:07:04.129
of this are 4 by 1 equals this is a 4 by

167
00:06:59.899 --> 00:07:07.129
3 matrix times a 3 by 1 vector plus a 4

168
00:07:04.129 --> 00:07:10.309
by 1 vector B and this is 4 by 1 same

169
00:07:07.129 --> 00:07:13.909
dimensions and remember that we said X

170
00:07:10.309 --> 00:07:17.389
is equal to a 0 right just like Y hat is

171
00:07:13.909 --> 00:07:19.549
also equal to a 2 so if you want you can

172
00:07:17.389 --> 00:07:23.389
actually take this X and replace it with

173
00:07:19.549 --> 00:07:25.519
a 0 since a 0 is if you want as an alias

174
00:07:23.389 --> 00:07:27.529
for the vector of input futures X now

175
00:07:25.519 --> 00:07:30.199
through a similar derivation you can

176
00:07:27.529 --> 00:07:32.569
figure out that the representation for

177
00:07:30.199 --> 00:07:34.999
the next layer can also be written

178
00:07:32.569 --> 00:07:38.389
similarly where what the output layer

179
00:07:34.999 --> 00:07:41.899
does is it has associated with it so the

180
00:07:38.389 --> 00:07:44.689
parameters W 2 and B 2 so W 2 in this

181
00:07:41.899 --> 00:07:46.999
case is going to be a 1 by 4 matrix and

182
00:07:44.689 --> 00:07:49.969
B 2 is just a real number as 1 by 1 and

183
00:07:46.999 --> 00:07:52.549
so V 2 is going to be a real numbers

184
00:07:49.969 --> 00:07:56.449
right as a 1 by 1 matrix is going to be

185
00:07:52.549 --> 00:07:59.059
a 1 by 4 thing times a was 4 by 1 plus B

186
00:07:56.449 --> 00:08:00.619
2 is 1 by 1 and so this gives you just a

187
00:07:59.059 --> 00:08:03.349
real number and if you think of this

188
00:08:00.619 --> 00:08:04.969
loss output unit as just being analogous

189
00:08:03.349 --> 00:08:09.499
to logistic regression which had

190
00:08:04.969 --> 00:08:13.219
parameters W and B on W really plays in

191
00:08:09.499 --> 00:08:16.279
nablus role to W 2 transpose or W 2's

192
00:08:13.219 --> 00:08:18.709
really W transpose and B is equal to B 2

193
00:08:16.279 --> 00:08:21.199
right similar to you know cover up the

194
00:08:18.709 --> 00:08:23.299
left of this network and ignore all that

195
00:08:21.199 --> 00:08:25.339
for now then this is just this last

196
00:08:23.299 --> 00:08:27.289
output unit there's a lot like logistic

197
00:08:25.339 --> 00:08:29.599
regression except that instead of

198
00:08:27.289 --> 00:08:32.779
writing the parameters as WMV we're

199
00:08:29.599 --> 00:08:36.079
writing them as W 2 and B 2 with

200
00:08:32.779 --> 00:08:38.809
dimensions one by four and one by one so

201
00:08:36.079 --> 00:08:41.059
just a recap for logistic regression to

202
00:08:38.809 --> 00:08:43.519
implement the output or the influence

203
00:08:41.059 --> 00:08:47.809
prediction you compute Z equals W

204
00:08:43.519 --> 00:08:50.790
transpose X plus B and a y hat equals a

205
00:08:47.809 --> 00:08:53.520
equals sigmoid of z

206
00:08:50.790 --> 00:08:55.170
when you have a new network who have one

207
00:08:53.520 --> 00:08:58.050
fit in there what you need to implement

208
00:08:55.170 --> 00:09:00.270
two computers output is just the four

209
00:08:58.050 --> 00:09:02.880
equation and you can think of this as a

210
00:09:00.270 --> 00:09:05.700
vectorized implementation of computing

211
00:09:02.880 --> 00:09:07.530
the output of first these four

212
00:09:05.700 --> 00:09:09.420
logistical russian units and hitting

213
00:09:07.530 --> 00:09:12.060
there that's what this does and then

214
00:09:09.420 --> 00:09:13.650
this which is regression in the output

215
00:09:12.060 --> 00:09:15.720
layer which is what this does

216
00:09:13.650 --> 00:09:18.120
I hope this description made sense but

217
00:09:15.720 --> 00:09:20.010
takeaway is to compute the output of

218
00:09:18.120 --> 00:09:22.500
this neural network all you need is

219
00:09:20.010 --> 00:09:25.050
those four lines of code so now you've

220
00:09:22.500 --> 00:09:27.360
seen how given a single input feature

221
00:09:25.050 --> 00:09:29.250
vector at you can with four lines of

222
00:09:27.360 --> 00:09:31.590
code compute the outputs of this viewer

223
00:09:29.250 --> 00:09:33.390
Network um similar to what we did for

224
00:09:31.590 --> 00:09:35.550
logistic regression will also want to

225
00:09:33.390 --> 00:09:38.580
vectorize across multiple training

226
00:09:35.550 --> 00:09:40.710
examples and we'll see that by stacking

227
00:09:38.580 --> 00:09:42.330
up training examples in different colors

228
00:09:40.710 --> 00:09:44.970
in the matrix or just slight

229
00:09:42.330 --> 00:09:46.830
modification to this you also similar to

230
00:09:44.970 --> 00:09:49.350
what you saw in which is regression be

231
00:09:46.830 --> 00:09:51.240
able to compute the output of this

232
00:09:49.350 --> 00:09:53.220
neural network not just on one example

233
00:09:51.240 --> 00:09:55.770
at a time belong your say your

234
00:09:53.220 --> 00:09:59.480
anti-trade set at a time so let's see

235
00:09:55.770 --> 00:09:59.480
the details of that in the next video