In addition to L2 regularization and
drop out regularization there are few other techniques to reducing
over fitting in your neural network. Let's take a look. Let's say you fitting a CAD crossfire. If you are over fitting getting more
training data can help, but getting more training data can be expensive and
sometimes you just can't get more data. But what you can do is augment your
training set by taking image like this. And for example,
flipping it horizontally and adding that also with your training set. So now instead of just this one
example in your training set, you can add this to your training example. So by flipping the images horizontally, you could double the size
of your training set. Because you're training set is now a bit
redundant this isn't as good as if you had collected an additional set of
brand new independent examples. But you could do this Without needing
to pay the expense of going out to take more pictures of cats. And then other than flipping horizontally, you can also take random
crops of the image. So here we're rotated and
sort of randomly zoom into the image and this still looks like a cat. So by taking random distortions and
translations of the image you could augment your data set and
make additional fake training examples. Again, these extra fake training examples
they don't add as much information as they were to call they get a brand new
independent example of a cat. But because you can do this,
almost for free, other than for some confrontational costs. This can be an inexpensive way to
give your algorithm more data and therefore sort of regularize it and
reduce over fitting. And by synthesizing examples like this
what you're really telling your algorithm is that If something is a cat then
flipping it horizontally is still a cat. Notice I didn't flip it vertically, because maybe we don't want
upside down cats, right? And then also maybe randomly zooming
in to part of the image it's probably still a cat. For optical character recognition you can
also bring your data set by taking digits and imposing random rotations and
distortions to it. So If you add these things
to your training set, these are also still digit force. For illustration I applied
a very strong distortion. So this look very wavy for, in practice
you don't need to distort the four quite as aggressively, but just a more subtle
distortion than what I'm showing here, to make this example clearer for
you, right? But a more subtle distortion
is usually used in practice, because this looks like
really warped fours. So data augmentation can be used
as a regularization technique, in fact similar to regularization. There's one other technique that is
often used called early stopping. So what you're going to do is as you run
gradient descent you're going to plot your, either the training error, you'll use 01 classification
error on the training set. Or just plot the cost
function J optimizing, and that should decrease monotonically,
like so, all right? Because as you trade, hopefully, you're trading around your cost
function J should decrease. So with early stopping,
what you do is you plot this, and you also plot your dev set error. And again, this could be a classification
error in a development sense, or something like the cost function, like the logistic
loss or the log loss of the dev set. Now what you find is that your dev
set error will usually go down for a while, and
then it will increase from there. So what early stopping does is,
you will say well, it looks like your neural network was
doing best around that iteration, so we just want to stop trading on
your neural network halfway and take whatever value achieved
this dev set error. So why does this work? Well when you've haven't
run many iterations for your neural network yet
your parameters w will be close to zero. Because with random initialization you
probably initialize w to small random values so before you train for
a long time, w is still quite small. And as you iterate, as you train, w will
get bigger and bigger and bigger until here maybe you have a much larger value of
the parameters w for your neural network. So what early stopping does is by
stopping halfway you have only a mid-size rate w. And so similar to L2 regularization by
picking a neural network with smaller norm for your parameters w, hopefully
your neural network is over fitting less. And the term early stopping refers
to the fact that you're just stopping the training of
your neural network earlier. I sometimes use early stopping
when training a neural network. But it does have one downside,
let me explain. I think of the machine learning process
as comprising several different steps. One, is that you want an algorithm
to optimize the cost function j and we have various tools to do that,
such as grade intersect. And then we'll talk later about
other algorithms, like momentum and RMS prop and Atom and so on. But after optimizing the cost function j,
you also wanted to not over-fit. And we have some tools to do that
such as your regularization, getting more data and so on. Now in machine learning, we already have
so many hyper-parameters it surge over. It's already very complicated to choose
among the space of possible algorithms. And so I find machine learning
easier to think about when you have one set of tools for
optimizing the cost function J, and when you're focusing on
authorizing the cost function J. All you care about is finding w and b,
so that J(w,b) is as small as possible. You just don't think about anything
else other than reducing this. And then it's completely
separate task to not over fit, in other words, to reduce variance. And when you're doing that, you have
a separate set of tools for doing it. And this principle is sometimes
called orthogonalization. And there's this idea, that you want to be
able to think about one task at a time. I'll say more about orthorganization
in a later video, so if you don't fully get the concept yet,
don't worry about it. But, to me the main downside
of early stopping is that this couples these two tasks. So you no longer can work on
these two problems independently, because by stopping gradient decent early, you're sort of breaking whatever you're
doing to optimize cost function J, because now you're not doing a great
job reducing the cost function J. You've sort of not done that that well. And then you also simultaneously
trying to not over fit. So instead of using different
tools to solve the two problems, you're using one that
kind of mixes the two. And this just makes the set of things you could try are more
complicated to think about. Rather than using early stopping, one
alternative is just use L2 regularization then you can just train the neural
network as long as possible. I find that this makes the search space
of hyper parameters easier to decompose, and easier to search over. But the downside of this though is that
you might have to try a lot of values of the regularization parameter lambda. And so this makes searching over many
values of lambda more computationally expensive. And the advantage of early stopping is
that running the gradient descent process just once, you get to try out
values of small w, mid-size w, and large w, without needing to try a lot
of values of the L2 regularization hyperparameter lambda. If this concept doesn't completely make
sense to you yet, don't worry about it. We're going to talk about
orthogonalization in greater detail in a later video,
I think this will make a bit more sense. Despite it's disadvantages,
many people do use it. I personally prefer to just
use L2 regularization and try different values of lambda. That's assuming you can afford
the computation to do so. But early stopping does let you
get a similar effect without needing to explicitly try lots
of different values of lambda. So you've now seen how to use data
augmentation as well as if you wish early stopping in order to reduce variance or
prevent over fitting your neural network. Next let's talk about some techniques for setting up your optimization problem
to make your training go quickly.